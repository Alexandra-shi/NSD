{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://habr.com/ru/post/346198/'\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['title', 'text', 'year', 'tags', 'votes', 'views', 'bookmarks', 'comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.find(\"div\",\n",
    "    class_=\"post__text post__text-html\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, '', raw_html)\n",
    "  regex = re.compile('[^A-Za-zА-Яа-я]')\n",
    "  cleantext = regex.sub(' ', cleantext)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document(id):\n",
    "    page = requests.get('https://habr.com/ru/post/' + str(id) + '/')\n",
    "    soup = BeautifulSoup(page.text, 'html5lib')\n",
    "    info = []\n",
    "    if soup.find(\"span\", class_= \"post__title-text\"):\n",
    "        #title\n",
    "        info.append(soup.find(\"span\", class_= \"post__title-text\").text)\n",
    "        #text\n",
    "        text = soup.find(\"div\", class_=\"post__text post__text-html\").text\n",
    "        info.append(cleanhtml(text))\n",
    "        #year\n",
    "        info.append(soup.find(\"span\", class_= \"post__time\").text.split()[2])\n",
    "        #tags\n",
    "        tagsshit = soup.find_all(\"a\", class_=\"inline-list__item-link hub-link\")\n",
    "        tags = []\n",
    "        for i in range(len(tagsshit)):\n",
    "            tags.append(tagsshit[i].text)\n",
    "        info.append(tags)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isvotes(tag):\n",
    "    return tag.clas and tag.has_attr('onclick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"bmenu__conversion\" href=\"https://habr.com/sandbox/start/\" onclick=\"if (typeof ga === 'function') { ga('send', 'event', 'habr_top_panel', 'become_an_author'); }\">Как стать автором</a>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(isvotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python',\n",
       " 'Визуализация данных',\n",
       " 'Исследования и прогнозы в IT',\n",
       " 'Веб-аналитика']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagsshit = soup.find_all(\"a\", class_=\"inline-list__item-link hub-link\")\n",
    "tags = []\n",
    "for i in range(len(tagsshit)):\n",
    "    tags.append(tagsshit[i].text)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"post__text post__text-html\" data-io-article-url=\"https://habr.com/ru/post/346198/\" id=\"post-content-body\"><p>Когда был доеден новогодний оливье, мне стало нечего делать, и я решил скачать себе на компьютер все статьи с Хабрахабра (и смежных платформ) и поисследовать. </p><br/>\n",
       "<p>Получилось несколько интересных сюжетов. Первый из них — это развитие формата и тематики статей за 12 лет существования сайта. Например, достаточно показательна динамика некоторых тем. Продолжение — под катом.</p><br/>\n",
       "<img src=\"https://habrastorage.org/webt/fa/ht/ez/fahtezsb64hlor2mkatmlp4q7mq.png\" width=\"540\"/><a name=\"habracut\"></a><br/>\n",
       "<h2 id=\"process-parsinga\">Процесс парсинга</h2><br/>\n",
       "<p>Чтобы понять, как развивался Хабр, нужно было обойти по все его статьи и выделить из них метаинформацию (например, даты). Обход дался легко, потому что ссылки на все статьи имеют вид \"habrahabr.ru/post/337722/\", причём номера задаются строго по порядку. Зная, что последний пост имеет номер чуть меньше 350 тысяч, я просто прошёлся по всем возможным id документов циклом (код на Python):</p><br/>\n",
       "<pre><code class=\"python\">import numpy as np\n",
       "from multiprocessing import Pool\n",
       "with Pool(100) as p:\n",
       "    docs = p.map(download_document, np.arange(350000))</code></pre><br/>\n",
       "<p>Функция <code>download_document</code> пытается загружает страницу с соответствующим id и пытается вытащить из структуры html содержательную информацию. </p><br/>\n",
       "<pre><code class=\"python\">import requests\n",
       "from bs4 import BeautifulSoup\n",
       "\n",
       "def download_document(pid):\n",
       "    \"\"\" Download and process a Habr document and its comments \"\"\"\n",
       "    # выгрузка документа\n",
       "    r = requests.get('https://habrahabr.ru/post/' +str(pid) + '/')\n",
       "    # парсинг документа\n",
       "    soup = BeautifulSoup(r.text, 'html5lib') # instead of html.parser\n",
       "    doc = {}\n",
       "    doc['id'] = pid\n",
       "    if not soup.find(\"span\", {\"class\": \"post__title-text\"}):\n",
       "        # такое бывает, если статья не существовала или удалена\n",
       "        doc['status'] = 'title_not_found'\n",
       "    else:\n",
       "        doc['status'] = 'ok'\n",
       "        doc['title'] = soup.find(\"span\", {\"class\": \"post__title-text\"}).text\n",
       "        doc['text'] = soup.find(\"div\", {\"class\": \"post__text\"}).text\n",
       "        doc['time'] = soup.find(\"span\", {\"class\": \"post__time\"}).text\n",
       "        # create other fields: hubs, tags, views, comments, votes, etc.\n",
       "        # ...\n",
       "    # сохранение результата в отдельный файл\n",
       "    fname = r'files/' + str(pid) + '.pkl'\n",
       "    with open(fname, 'wb') as f:\n",
       "        pickle.dump(doc, f)</code></pre><br/>\n",
       "<p>В процессе парсинга открыл для себя несколько новых моментов. </p><br/>\n",
       "<p>Во-первых, говорят, что создавать больше процессов, чем ядер в процессоре, бесполезно. Но в моём случае оказалось, что лимитирующий ресурс — не процессор, а сеть, и 100 процессов отрабатывают быстрее, чем 4 или, скажем, 20. </p><br/>\n",
       "<p>Во-вторых, в некоторых постах встречались сочетания спецсимволов — например, эвфемизмы типа \"%&amp;#@\". Оказалось, что <code>html.parser</code>, который я использовал сначала, реагирует на комбинацию <code>&amp;#</code> болезненно, считая её началом html-сущности. Я уж было собирался творить чёрную магию, но на форуме подсказали, что можно просто поменять парсер.</p><br/>\n",
       "<p>В-третьих, мне удалось выгрузить все публикации, кроме трёх. Документы под номерами 65927, 162075, и 275987 моментально удалил мой антивирус. Это статьи соответственно про цепочку джаваскриптов, загружающую зловредный pdf, SMS-вымогатель в виде набора плагинов для браузеров, и сайт CrashSafari.com, который отправляет айфоны в перезагрузку. Ещё одну статью антивирь обнаружил позднее, во время скана системы: пост 338586 про скрипты на сайте зоомагазина, использующие процессор пользователя для майнинга криптовалюты. Так что можно считать работу антивируса вполне адекватной. </p><br/>\n",
       "<p>\"Живых\" статей оказалась только половина от потенциального максимума — 166307 штук. Про остальные Хабр даёт варианты \"страница устарела, была удалена или не существовала вовсе\". Что ж, всякое бывает. </p><br/>\n",
       "<p>За выгрузкой статей последовала техническая работа: например, даты публикации нужно было перевести из формата \"'21 декабря 2006 в 10:47\" в стандартный <code>datetime</code>, а \"12,8k\" просмотров — в 12800. На этом этапе вылезло ещё несколько казусов. Самый весёлый связан с подсчётом голосов и типами данных: в некоторых старых постах произошло переполнение инта, и они получили по 65535 голосов.</p><br/>\n",
       "<img src=\"https://habrastorage.org/webt/ga/a1/0x/gaa10x2xrj6cz9y5mjouf6qghaw.png\" width=\"240\"/><br/>\n",
       "<p>В результате тексты статей (без картинок) заняли у меня 1.5 гигабайта, комментарии с метаинформацией — ещё 3, и около сотни мегабайт — метаинформация о статьях. Такое можно полностью держать в оперативной памяти, что было для меня приятной неожиданностью. </p><br/>\n",
       "<p>Начал анализ статей я не с самих текстов, а с метаинформации: дат, тегов, хабов, просмотров и \"лайков\". Оказалось, что и она может многое поведать. </p><br/>\n",
       "<h2 id=\"trendy-razvitiya-habrahabra\">Тренды развития Хабрахабра</h2><br/>\n",
       "<p>Статьи на сайте публикуются с 2006 года; наиболее интенсивно — в 2008-2016 годах. </p><br/>\n",
       "<img src=\"https://habrastorage.org/webt/5x/xq/wy/5xxqwy0ozkxidpiunlp59wzstzm.png\" width=\"360\"/><br/>\n",
       "<p>Насколько активно эти статьи читали в разное время, оценить не так просто. Тексты 2012 года и младше более активно комментировали и рейтинговали, но у более новых текстов больше просмотров и добавлений в закладки. Одинаково вели себя (вдвое упали) эти метрики только однажды, в 2015 году. Возможно, в ситуации экономического и политического кризиса внимание читателей перешло с айтишных блогов к более болезненным вопросам. </p><br/>\n",
       "<img src=\"https://habrastorage.org/webt/ka/tl/nh/katlnhwb2odlxk3p6uccsqwldcw.png\" width=\"360\"/><br/>\n",
       "<p>Кроме самих статей, я выкачал ещё комментарии к ним. Комментариев получилось 6 миллионов, правда, 240 тысяч из них оказались забаненными (\"нло прилетело и опубликовало эту надпись здесь\"). Полезное свойство комментариев в том, что для них указано время. Изучая время комментариев, можно примерно понять и то, когда вообще статьи читают. </p><br/>\n",
       "<p>Оказалось, что большую часть статей и пишут, и комментируют где-то с 10 до 20 часов, т.е. в типичный московский рабочий день. Это может значить и что Хабр читают в профессиональных целях, и что это хороший способ прокрастинации на работе. Кстати, это распределение времени суток стабильно с самого основания Хабра до сегодняшнего дня. </p><br/>\n",
       "<img src=\"https://habrastorage.org/webt/ck/zv/vu/ckzvvuqwr1mnhs_0hvvw0qdaqqs.png\" width=\"360\"/><br/>\n",
       "<p>Однако основная польза от метки времени комментария — не время суток, а срок \"активной жизни\" статьи. Я подсчитал, как распределено время от публикации статьи до её комментария. Оказалось, что сейчас медианный комментарий (зелёная линия) приходит примерно через 20 часов, т.е. в первые сутки после публикации оставляют в среднем чуть больше половины всех комментариев к статье. А за двое суток оставляют 75% всех комментариев. При этом раньше статьи читали ещё быстрее — так, в 2010 году половина комментариев приходила уже в первые 6 часов. </p><br/>\n",
       "<img src=\"https://habrastorage.org/webt/ce/gj/k9/cegjk9fpdcpi7gqijes6dpzvwle.png\" width=\"360\"/><br/>\n",
       "<p>Для меня стало сюрпризом, что комментарии удлинились: среднее количество символов в комментарии за время существования Хабра выросло почти вдвое!</p><br/>\n",
       "<img src=\"https://habrastorage.org/webt/fn/ay/jl/fnayjlfpsyf3vf2wsj8vyb5g2hm.png\" width=\"360\"/><br/>\n",
       "<p>Более простая обратная связь, чем комментарии — это голоса. В отличие от многих других ресурсов, на Хабре можно ставить не только плюсы, но и минусы. Впрочем, последней возможностью читатели пользуются не так часто: текущая доля дизлайков составляет около 15% от всех отданных голосов. Раньше было больше, но со временем читатели подобрели. </p><br/>\n",
       "<img src=\"https://habrastorage.org/webt/k3/_t/je/k3_tjevzru_8-tez48n0zh-hiog.png\" width=\"360\"/><br/>\n",
       "<p>Менялись со временем и сами тексты. Например, типичная длина текста не прекращает устойчиво расти с самого запуска сайта, несмотря на кризисы. За десятилетие тексты стали почти в десять раз длиннее!</p><br/>\n",
       "<img src=\"https://habrastorage.org/webt/0q/ji/cp/0qjicpigcx6k2aeyeak0ob-wny0.png\" width=\"360\"/><br/>\n",
       "<p>Стилистика текстов (в первом приближении) тоже менялась. За первые годы существования Хабра, например, выросла доля кода и чисел в текстах:</p><br/>\n",
       "<img src=\"https://habrastorage.org/webt/v3/wo/lw/v3wolwgn1nycharjflckycowj94.png\" width=\"720\"/><br/>\n",
       "<p>Разобравшись с общей динамикой сайта, я решил измерить, как менялась популярность различных тем. Темы можно выделять из текстов автоматически, но для начала можно не изобретать велосипед, а воспользоваться готовыми тегами, проставленными авторами каждой статьи. Четыре типичных тренда я вывел на графике. Тема \"Google\" изначально доминировала (возможно, в основном в связи с SEO-оптимизацией), но с каждым годом теряла вес. Javascript был популярной темой и продолжает постепенно, а вот машинное обучение начало стремительно набирать популярность лишь в последние годы. Linux же остаётся одинаково актуальным на протяжении всего десятилетия. </p><br/>\n",
       "<img src=\"https://habrastorage.org/webt/vx/s5/yy/vxs5yy1ojnjf4hlufmoi4j9fsve.png\" width=\"360\"/><br/>\n",
       "<p>Конечно же, мне стало интересно, какие темы привлекают больше читательской активности. Я подсчитал медианное число просмотров, голосов и комментов в каждой теме. Вот что получилось:</p><br/>\n",
       "<ul>\n",
       "<li>Самые просматриваемые темы: arduino, веб-дизайн, веб-разработка, дайджест, ссылки, css, html, html5, nginx, алгоритмы. </li>\n",
       "<li>Самые \"лайкабельные\" темы: вконтакте, юмор, jquery, opera, c, html, веб-разработка, html5, css, веб-дизайн. </li>\n",
       "<li>Самые обсуждаемые темы: opera, skype, фриланс, вконтакте, ubuntu, работа, nokia, nginx, arduino, firefox. </li>\n",
       "</ul><br/>\n",
       "<p>Кстати, раз уж я сравниваю темы, можно сделать их рейтинг по частоте (и сравнить результаты с <a href=\"http://habrahabr.ru/post/197308/\" rel=\"nofollow\">аналогичной статьёй от 2013 года</a>). </p><br/>\n",
       "<ul>\n",
       "<li>За все годы существования Хабра самыми популярными тегами (в порядке убывания) стали google, android, javascript, microsoft, linux, php, apple, java, python, программирование, стартапы, разработка, ios, стартап, социальные сети</li>\n",
       "<li>В 2017 году наиболее популярны были javascript, python, java, android, разработка, linux, c++, программирование, php, c#, ios, машинное обучение, информационная безопасность, microsoft, react</li>\n",
       "</ul><br/>\n",
       "<p>При сравнении этих рейтингов можно обратить внимание, например, на победоносное шествие Питона и вымирание php, или на \"закат\" стартаперской тематики и взлёт машинного обучения.</p><br/>\n",
       "<p>Не все теги на Хабре имеют столь очевидную тематическую окраску. Вот, например, десяток тегов, которые встречались всего лишь один раз, но просто показались мне забавными. Итак: \"идея движитель прогресса\", \"загрузка с образа дискеты\", \"штат айова\", \"драматургия\", \"супералеша\", \"паровой двигатель\", \"чем заняться в субботу\", \"у меня лисица в мясорубке\", \"а получилось как всегда\", \"смешных тэгов придумать не удалось\". Чтобы определить тематику таких статей, тегов недостаточно — придётся осуществлять тематическое моделирование над текстами статей. </p><br/>\n",
       "<p>Более подробный анализ содержания статей будет в следующем посте. Во-первых, я собираюсь построить модель, прогнозирующую количество просмотров статьи в зависимости от её содержания. Во-вторых, хочется научить нейросеть генерировать тексты в той же стилистике, что и у авторов Хабра. Так что подписывайтесь :)</p></div>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"div\", class_=\"post__text post__text-html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8 января 2018 в 15:54'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"span\", {\"class\": \"post__time\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
